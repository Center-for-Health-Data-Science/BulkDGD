

<!DOCTYPE html>


<html lang="en" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>The core subpackage &#8212; BulkDGD  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'core';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="The utils subpackage" href="utils.html" />
    <link rel="prev" title="Command-line interface" href="Command-line%20interface.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    <p class="title logo__title">BulkDGD  documentation</p>
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="Getting%20started.html">
                        Getting started
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="Installation.html">
                        Installing bulkDGD
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="Command-line%20interface.html">
                        Command-line interface
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="#">
                        The core subpackage
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="utils.html">
                        The utils subpackage
                      </a>
                    </li>
                
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="Getting%20started.html">
                        Getting started
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="Installation.html">
                        Installing bulkDGD
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="Command-line%20interface.html">
                        Command-line interface
                      </a>
                    </li>
                

                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="#">
                        The core subpackage
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="utils.html">
                        The utils subpackage
                      </a>
                    </li>
                
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumbs">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Breadcrumb">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">The core subpackage</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="the-core-subpackage">
<h1>The core subpackage<a class="headerlink" href="#the-core-subpackage" title="Permalink to this heading">#</a></h1>
<section id="core-dataclasses">
<h2>core.dataclasses<a class="headerlink" href="#core-dataclasses" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="core.dataclasses.GeneExpressionDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">core.dataclasses.</span></span><span class="sig-name descname"><span class="pre">GeneExpressionDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.dataclasses.GeneExpressionDataset" title="Permalink to this definition">#</a></dt>
<dd><p>Class implementing a dataset containing gene expression data
for multiple samples, so that one can take advantage of the
<code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> utility for a map-style dataset.</p>
<p>The class needs to implement a <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> and a <code class="docutils literal notranslate"><span class="pre">__len__</span></code>
method.</p>
<dl class="py method">
<dt class="sig sig-object py" id="core.dataclasses.GeneExpressionDataset.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">df</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.dataclasses.GeneExpressionDataset.__init__" title="Permalink to this definition">#</a></dt>
<dd><p>Initialize an instance of the class.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>df</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">pandas.DataFrame</span></code></span></dt><dd><p>A data frame whose rows must represent samples,
and columns must represent genes. Therefore, each
cell of the data frame represents the expression
of the gene on the column in the sample on the row.</p>
<p>For example:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>,gene_1,gene_2,gene_3,gene_4
sample_1,123,12,2342,145
sample_2,189,184,2397,1980
sample_3,978,9467,563,23
</pre></div>
</div>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.dataclasses.GeneExpressionDataset.df">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">df</span></span><a class="headerlink" href="#core.dataclasses.GeneExpressionDataset.df" title="Permalink to this definition">#</a></dt>
<dd><p>The original data frame from which the dataset is
constructed.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.dataclasses.GeneExpressionDataset.mean_exp">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean_exp</span></span><a class="headerlink" href="#core.dataclasses.GeneExpressionDataset.mean_exp" title="Permalink to this definition">#</a></dt>
<dd><p>A one-dimensional <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> with length equal
to the number of samples in the dataset containing the
mean gene expression for each sample.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.dataclasses.GeneExpressionDataset.n_genes">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n_genes</span></span><a class="headerlink" href="#core.dataclasses.GeneExpressionDataset.n_genes" title="Permalink to this definition">#</a></dt>
<dd><p>The number of genes in the data frame.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.dataclasses.GeneExpressionDataset.n_samples">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n_samples</span></span><a class="headerlink" href="#core.dataclasses.GeneExpressionDataset.n_samples" title="Permalink to this definition">#</a></dt>
<dd><p>The number of samples in the dataset.</p>
</dd></dl>

</dd></dl>

</section>
<section id="core-decoder">
<h2>core.decoder<a class="headerlink" href="#core-decoder" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="core.decoder.NBLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">core.decoder.</span></span><span class="sig-name descname"><span class="pre">NBLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r_init</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'softplus'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.decoder.NBLayer" title="Permalink to this definition">#</a></dt>
<dd><p>Class implementing an output layer representing the means
of the negative binomial distributions modeling the outputs
(e.g., the means of the gene expression counts). One negative
binomial distribution with trainable parameters is used for
each gene.</p>
<p>The negative binomial models the number of “successes” in a
sequence of independent and identically distributed Bernoulli
trials before a specified number of “failures” occur.</p>
<p>A Bernoulli trial is a trial where there are only two
possible mutually exclusive outcomes.</p>
<dl class="py method">
<dt class="sig sig-object py" id="core.decoder.NBLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r_init</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'softplus'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.decoder.NBLayer.__init__" title="Permalink to this definition">#</a></dt>
<dd><p>Initialize an instance of the class.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>dim</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The dimensionality of the negative binomial
distributions.</p>
</dd>
<dt><strong>r_init</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The initial value for <code class="docutils literal notranslate"><span class="pre">r</span></code>, representing the “number
of failures” after which the “trials” stop.</p>
</dd>
<dt><strong>activation</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;softplus&quot;</span></code></span></dt><dd><p>The name of the activation function to be used.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.decoder.NBLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.decoder.NBLayer.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Forward pass. Pass the input tensor through the
activation function and return the result.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The input tensor.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></dt><dd><p>A tensor containing the result of passing the input
tensor through the activation function. This tensor
has the same shape as the input tensor.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.decoder.NBLayer.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs_count</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaling_factor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.decoder.NBLayer.log_prob" title="Permalink to this definition">#</a></dt>
<dd><p>Get the log-likelihood of an input <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>obs_count</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The observed gene counts.</p>
</dd>
<dt><strong>scaling_factor</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>A tensor containing the scaling factor(s). It must have the
same dimensionality as <code class="docutils literal notranslate"><span class="pre">obs_count</span></code>, and
the size of the first dimension must match that of the
first dimension of <code class="docutils literal notranslate"><span class="pre">obs_count</span></code>.</p>
</dd>
<dt><strong>pred_mean</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The predicted means of the negative binomials. This is a
tensor whose shape must match that of <code class="docutils literal notranslate"><span class="pre">obs_count</span></code> and
<code class="docutils literal notranslate"><span class="pre">scaling_factor</span></code>.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></dt><dd><p>The log-likelihood of the input.</p>
<p>This tensor contains a value for each of the negative
binomials associated with the values in <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.decoder.NBLayer.log_prob_mass">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">log_prob_mass</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">m</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.decoder.NBLayer.log_prob_mass" title="Permalink to this definition">#</a></dt>
<dd><p>Compute the logarithm of the probability density
for a set of negative binomial distribution.</p>
<p>Thr formula used to compute the logarithm of the
probability density is:</p>
<div class="math notranslate nohighlight">
\[logPDF_{NB(k,m,r)} =
log\Gamma(k+r) - log\Gamma(r) - log\Gamma(k+1) +
k \cdot log(m \cdot c + \epsilon) +
r \cdot log(r \cdot c)\]</div>
<p>Where <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small value to prevent
underflow/overflow, and <span class="math notranslate nohighlight">\(c\)</span> is equal to
<span class="math notranslate nohighlight">\(\frac{1}{r+m+\epsilon}\)</span>.</p>
<p>The derivation of this formula from the non-logarithmic
formulation of the probability density function of the
negative binomial distribution can be found below.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>k</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>“Number of successes” seen before
stopping the trials (each value in the
tensor corresponds to the number of successes
of a different negative binomial).</p>
</dd>
<dt><strong>m</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>Means of the negative binomials (each value in
the tensor corresponds to the mean of a
different negative binomial).</p>
</dd>
<dt><strong>r</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>“Number of failures” after which the trials
end (each value in the tensor corresponds to
the number of failures of a different negative
binomial).</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>x</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The log-probability density of the negative binomials.
This is a 1D tensor whose length corresponds to the
number of negative binomials distributions considered,
and each value in the tensor corresponds to the
log-probability density of a different negative binomial.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Here, we show how we derived the formula for the logarithm
of the probability density of the negative binomial
distribution.</p>
<p>We start from the non-logarithmic version of the
probability density for the negative binomial, which is:</p>
<div class="math notranslate nohighlight">
\[PDF_{NB(k,m,r)} =            \binom{k+r-1}{k} (1-p)^{k} p^{r}\]</div>
<p>However, since:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(1-p\)</span> is equal to <span class="math notranslate nohighlight">\(\frac{m}{r+m}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(p\)</span> is equal to <span class="math notranslate nohighlight">\(\frac{r}{r+m}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(k+r-1\)</span> can be rewritten in terms of the
gamma function as <span class="math notranslate nohighlight">\(\Gamma(k+r)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(k\)</span> can also be rewritten as
<span class="math notranslate nohighlight">\(\Gamma(r) \cdot k!\)</span></p></li>
</ul>
<p>The formula becomes:</p>
<div class="math notranslate nohighlight">
\[PDF_{NB(k,m,r)} =            \binom{\Gamma(k+r)}{\Gamma(r) \cdot k!}
\left( \frac{m}{r+m} \right)^k
\left( \frac{r}{r+m} \right)^r\]</div>
<p>However, <span class="math notranslate nohighlight">\(k!\)</span> can be also be rewritten as
<span class="math notranslate nohighlight">\(\Gamma(k+1)\)</span>, resulting in:</p>
<div class="math notranslate nohighlight">
\[PDF_{NB(k,m,r)} =            \binom{\Gamma(k+r)}{\Gamma(r) \cdot 
\Gamma(k+1)}
\left( \frac{m}{r+m} \right)^k
\left( \frac{r}{r+m} \right)^r\]</div>
<p>Then, we get the natural logarithm:</p>
<div class="math notranslate nohighlight">
\[logPDF_{NB(k,m,r)} =            log\Gamma(k+r) - log\Gamma(r) - log\Gamma(k+1) +
k \cdot log \left( \frac{m}{r+m} \right) +
r \cdot log \left( \frac{r}{r+m} \right)\]</div>
<p>Here, we are adding a small value <span class="math notranslate nohighlight">\(\epsilon\)</span> to
prevent underflow/overflow:</p>
<div class="math notranslate nohighlight">
\[logPDF_{NB(k,m,r)} =            log\Gamma(k+r) - log\Gamma(r) - log\Gamma(k+1) +
k \cdot
log \left( m \cdot \frac{1}{r+m+\epsilon} 
+ \epsilon \right) +
r \cdot
log \left( r \cdot \frac{1}{r+m+\epsilon}
\right)\]</div>
<p>Finally, we substitute <span class="math notranslate nohighlight">\(\frac{1}{r+m+\epsilon}\)</span>
with <span class="math notranslate nohighlight">\(c\)</span> and we obtain:</p>
<div class="math notranslate nohighlight">
\[logPDF_{NB(k,m,r)} =            log\Gamma(k+r) - log\Gamma(r) - log\Gamma(k+1) +
k \cdot
log \left( m \cdot c + \epsilon \right) +
r \cdot
log \left( r \cdot c \right)\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.decoder.NBLayer.loss">
<span class="sig-name descname"><span class="pre">loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obs_count</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaling_factor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pred_mean</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.decoder.NBLayer.loss" title="Permalink to this definition">#</a></dt>
<dd><p>Compute the loss given observed means <code class="docutils literal notranslate"><span class="pre">obs_count</span></code> and
predicted means <code class="docutils literal notranslate"><span class="pre">pred_mean</span></code>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>obs_count</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The observed gene counts.</p>
</dd>
<dt><strong>scaling_factor</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>A tensor containing the scaling factor(s). It must have the
same dimensionality as <code class="docutils literal notranslate"><span class="pre">obs_count</span></code>, and
the size of the first dimension must match that of the
first dimension of <code class="docutils literal notranslate"><span class="pre">obs_count</span></code>.</p>
</dd>
<dt><strong>pred_mean</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The predicted means of the negative binomials. This is a
tensor whose shape must match that of <code class="docutils literal notranslate"><span class="pre">obs_count</span></code> and
<code class="docutils literal notranslate"><span class="pre">scaling_factor</span></code>.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></dt><dd><p>The loss associated to the input <code class="docutils literal notranslate"><span class="pre">x</span></code>.</p>
<p>This is a 2D tensor where:</p>
<ul class="simple">
<li><p>The first dimension has a length equal to the length
of the first dimension of <code class="docutils literal notranslate"><span class="pre">obs_count</span></code> and
<code class="docutils literal notranslate"><span class="pre">pred_mean</span></code>.</p></li>
<li><p>The second dimension has a length equal to the length
of the second dimension of <code class="docutils literal notranslate"><span class="pre">obs_count</span></code> and
<code class="docutils literal notranslate"><span class="pre">pred_mean</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.decoder.NBLayer.rescale">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">rescale</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scaling_factor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.decoder.NBLayer.rescale" title="Permalink to this definition">#</a></dt>
<dd><p>Rescale the mean of the means of the negative binomial
distributions by a per-batch scaling factor.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>scaling_factor</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The scaling factor.</p>
<p>This is a 1D tensor whose length is equal to the
number of scaling factors to be used to rescale
the means.</p>
</dd>
<dt><strong>mean</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>A 1D tensor containing the  means of the negative
binomials o be rescaled.</p>
<p>In the tensor, each value represents the 
mean of a different negative binomial.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></dt><dd><p>The rescaled means. This is a 1D tensor whose length
is equal to the number of negative binomials whose
means were rescaled.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.decoder.NBLayer.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scaling_factor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pred_mean</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.decoder.NBLayer.sample" title="Permalink to this definition">#</a></dt>
<dd><p>Get <code class="docutils literal notranslate"><span class="pre">n</span></code> samples from the negative binomials.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>n</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The number of samples to get.</p>
</dd>
<dt><strong>scaling_factor</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>A tensor containing the scaling factor(s).</p>
</dd>
<dt><strong>pred_mean</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The predicted means of the negative binomials. This is a
tensor whose shape must match that of  <code class="docutils literal notranslate"><span class="pre">scaling_factor</span></code>.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></dt><dd><p>The samples drawn from the negative binomial distributions.</p>
<p>The shape of this tensor depends on the shape of <code class="docutils literal notranslate"><span class="pre">n</span></code>
and <code class="docutils literal notranslate"><span class="pre">scaling_factor</span></code>, but the first dimension always
has a length equal to the number of samples drawn from
the negative binomial distribution.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.decoder.NBLayer.activation">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">activation</span></span><a class="headerlink" href="#core.decoder.NBLayer.activation" title="Permalink to this definition">#</a></dt>
<dd><p>The activation function used in the <code class="docutils literal notranslate"><span class="pre">NBLayer</span></code>,
which depends on the scaling type.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.decoder.NBLayer.dim">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dim</span></span><a class="headerlink" href="#core.decoder.NBLayer.dim" title="Permalink to this definition">#</a></dt>
<dd><p>The dimensionality of the <code class="docutils literal notranslate"><span class="pre">NBLayer</span></code>.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.decoder.NBLayer.log_r">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">log_r</span></span><a class="headerlink" href="#core.decoder.NBLayer.log_r" title="Permalink to this definition">#</a></dt>
<dd><p>The natural logarithm of the <code class="docutils literal notranslate"><span class="pre">r</span></code> values associated
with the negative binomial distributions (the “number
of failures” after which the “trials” end).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="core.decoder.Decoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">core.decoder.</span></span><span class="sig-name descname"><span class="pre">Decoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_neurons_input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_neurons_hidden1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_neurons_hidden2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_neurons_output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'softplus'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.decoder.Decoder" title="Permalink to this definition">#</a></dt>
<dd><p>Class implementing the decoder.</p>
<dl class="py method">
<dt class="sig sig-object py" id="core.decoder.Decoder.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_neurons_input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_neurons_hidden1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_neurons_hidden2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_neurons_output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'softplus'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.decoder.Decoder.__init__" title="Permalink to this definition">#</a></dt>
<dd><p>Initialize an instance of the neural network representing
the decoder.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>n_neurons_input</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The mumber of neurons in the input layer.</p>
</dd>
<dt><strong>n_neurons_hidden1</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The number of neurons in the first hidden layer.</p>
</dd>
<dt><strong>n_neurons_hidden2</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The number of neurons in the second hidden layer.</p>
</dd>
<dt><strong>n_neurons_output</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The number of neurons in the output layer.</p>
</dd>
<dt><strong>r_init</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The initial value for <code class="docutils literal notranslate"><span class="pre">r</span></code>, representing the “number
of failures” after which the “trials” stop in the
<code class="docutils literal notranslate"><span class="pre">NBLayer</span></code>.</p>
</dd>
<dt><strong>activation_output</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code></span></dt><dd><p>The name of the activation function to be used in
the output layer.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.decoder.Decoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.decoder.Decoder.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Perform the forward pass through the neural network.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>z</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>A tensor holding the representations to pass
through the decoder.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></dt><dd><p>A tensor holding the outputs of the decoder
for the given representations.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="core-latent">
<h2>core.latent<a class="headerlink" href="#core-latent" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">core.latent.</span></span><span class="sig-name descname"><span class="pre">GaussianMixtureModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_comp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean_prior</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cm_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'isotropic'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_var_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(0.5,</span> <span class="pre">0.5)</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.latent.GaussianMixtureModel" title="Permalink to this definition">#</a></dt>
<dd><p>A class implementing a mixture of multivariate Gaussians
(Gaussian mixture model or GMM).</p>
<dl class="py method">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_comp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean_prior</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cm_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'isotropic'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_var_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(0.5,</span> <span class="pre">0.5)</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.latent.GaussianMixtureModel.__init__" title="Permalink to this definition">#</a></dt>
<dd><p>Initialize an instance of the GMM.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>dim</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The dimensionality of the Gaussian mixture model.</p>
</dd>
<dt><strong>n_comp</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The number of components in the mixture.</p>
</dd>
<dt><strong>mean_prior</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">object</span></code></span></dt><dd><p>Instance of a class representing a prior.</p>
</dd>
<dt><strong>cm_type</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, {<code class="docutils literal notranslate"><span class="pre">&quot;fixed&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;isotropic&quot;</span></code>,         <code class="docutils literal notranslate"><span class="pre">&quot;diagonal&quot;</span></code>}, <code class="docutils literal notranslate"><span class="pre">&quot;isotropic&quot;</span></code></span></dt><dd><p>Shape of the covariance matrix.</p>
</dd>
<dt><strong>alpha</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">1</span></code></span></dt><dd><p>Alpha of the Dirichlet distribution determining
the uniformity of the weights of the components
in the mixture.</p>
</dd>
<dt><strong>log_var_params</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">tuple</span></code>, <code class="docutils literal notranslate"><span class="pre">(0.5,</span> <span class="pre">0.5)</span></code></span></dt><dd><p>Tuple containing the parameters to initialize the
Gaussian prior on the log-variances of the components:</p>
<ul class="simple">
<li><p>Mean: <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">log(log_var_params[0])</span></code></p></li>
<li><p>Standard deviation: <code class="docutils literal notranslate"><span class="pre">log_var_params[1]</span></code></p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.component_sample">
<span class="sig-name descname"><span class="pre">component_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_samples</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.latent.GaussianMixtureModel.component_sample" title="Permalink to this definition">#</a></dt>
<dd><p>Sample from each component of the GMM.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>n_samples</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The number of samples to be drawn for each component
of the Gaussian mixture.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>component_samples</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The sampled points from the components of the Gaussian
mixture. This is a 3D tensor where:</p>
<ul class="simple">
<li><p>The first dimension has a length equal to the number
of samples drawn from each component.</p></li>
<li><p>The second dimension has a length equal to the number
of components in the Gaussian mixture.</p></li>
<li><p>The third dimension has a length equal to the
dimensionality of the Gaussian mixture model.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.latent.GaussianMixtureModel.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Forward pass - compute the negative log-probability density
for a set of data points.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The input data points. This is a 2D tensor where:</p>
<ul class="simple">
<li><p>The first dimension has a length equal to the number
of data points.</p></li>
<li><p>The second dimension has a length equal to the
dimensionality of the data points.</p></li>
</ul>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>y</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>This is a 1D tensor whose size is equal to the number
of input data points.</p>
<p>Each element of the tensor is the negative log-probability
density of a data point.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.get_distribution">
<span class="sig-name descname"><span class="pre">get_distribution</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.latent.GaussianMixtureModel.get_distribution" title="Permalink to this definition">#</a></dt>
<dd><p>Create a distribution from the GMM for sampling.</p>
<dl class="field-list">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>dist</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.distributions.MixtureSameFamily</span></code></span></dt><dd><p>A distribution created from the GMM.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.get_mixture_probs">
<span class="sig-name descname"><span class="pre">get_mixture_probs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.latent.GaussianMixtureModel.get_mixture_probs" title="Permalink to this definition">#</a></dt>
<dd><p>Convert the weights into mixture probabilities using
the softmax function.</p>
<dl class="field-list">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>mixture_probs</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The mixture probabilities. This is a 1D tensor whose
size equals the number of components in the Gaussian
mixture.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.get_prior_log_prob">
<span class="sig-name descname"><span class="pre">get_prior_log_prob</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.latent.GaussianMixtureModel.get_prior_log_prob" title="Permalink to this definition">#</a></dt>
<dd><p>Calculate the log probability of the prior on the means,
log-variance, and mixture coefficients.</p>
<dl class="field-list">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>p</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">float</span></code></span></dt><dd><p>The probability of the prior.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.latent.GaussianMixtureModel.log_prob" title="Permalink to this definition">#</a></dt>
<dd><p>Get the log-probability density of the samples <code class="docutils literal notranslate"><span class="pre">x</span></code>
being drawn from the GMM.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The input data points. This is a 2D tensor where:</p>
<ul class="simple">
<li><p>The first dimension has a length equal to the number
of data points.</p></li>
<li><p>The second dimension has a length equal to the
dimensionality of the data points.</p></li>
</ul>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>log_prob</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>A 1D tensor storing the log-probability density of each
input data points to be drawn from the GMM. The tensor
has a size equal to the number of data points passed.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_samples</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.latent.GaussianMixtureModel.sample" title="Permalink to this definition">#</a></dt>
<dd><p>Create samples from the GMM distribution.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>n</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The number of samples to be drawn from the GMM distribution.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>samples</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The points sampled from the GMM distribution.</p>
<p>This is a 2D tensor where:</p>
<ul class="simple">
<li><p>The first dimension has a length equal to the number
of samples drawn.</p></li>
<li><p>The second dimension has a length equal to the
dimensionality of the samples drawn (= the
dimensionality of the Gaussian mixture model).</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.sample_new_points">
<span class="sig-name descname"><span class="pre">sample_new_points</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_points</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples_per_comp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampling_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'random'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.latent.GaussianMixtureModel.sample_new_points" title="Permalink to this definition">#</a></dt>
<dd><p>Draw samples for new data points from each component
of the Gaussian mixture model</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>n_points</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>Number of data points for which samples should be drawm.</p>
</dd>
<dt><strong>n_samples_per_comp</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code>, <code class="docutils literal notranslate"><span class="pre">1</span></code></span></dt><dd><p>Number of samples to draw per data point per component
of the Gaussian mixture.</p>
</dd>
<dt><strong>sampling_method</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, {<code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;random&quot;</span></code>},                           <code class="docutils literal notranslate"><span class="pre">&quot;random&quot;</span></code></span></dt><dd><p>How to draw the samples for the given data points:</p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">&quot;random&quot;</span></code> means sampling <code class="docutils literal notranslate"><span class="pre">n_samples_per_comp</span></code></dt><dd><p>vectors from each mixture component at random.</p>
</dd>
</dl>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code> means taking the mean of each component as
the value of each <code class="docutils literal notranslate"><span class="pre">n_samples_per_comp</span></code> sample taken
for each data point.</p></li>
</ul>
<p>Setting <code class="docutils literal notranslate"><span class="pre">sampling_method</span></code> to <code class="docutils literal notranslate"><span class="pre">random</span></code> is equivalent
to calling the <code class="docutils literal notranslate"><span class="pre">component_sample()</span></code> method with
<code class="docutils literal notranslate"><span class="pre">n_samples</span></code> set to
<code class="docutils literal notranslate"><span class="pre">n_points</span> <span class="pre">*</span> <span class="pre">n_samples_per_comp</span></code>.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>new_points</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The samples drawn. This is a 2D tensor where:</p>
<ul class="simple">
<li><p>The first dimension has a length equal to
<code class="docutils literal notranslate"><span class="pre">n_points</span> <span class="pre">*</span> <span class="pre">n_reps_per_mix_comp</span> <span class="pre">*</span> <span class="pre">n_comp</span></code>.</p></li>
<li><p>The second dimension has a length equal to the
dimensionality of the Gaussian mixture model.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.sample_probs">
<span class="sig-name descname"><span class="pre">sample_probs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.latent.GaussianMixtureModel.sample_probs" title="Permalink to this definition">#</a></dt>
<dd><p>Get the probability density per sample per component.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The input data points. This is a 2D tensor where:</p>
<ul class="simple">
<li><p>The first dimension has a length equal to the number
of data points.</p></li>
<li><p>The second dimension has a length equal to the
dimensionality of the data points.</p></li>
</ul>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><strong>probs</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The probability densities.</p>
<p>This is a 2D tensor where:</p>
<ul class="simple">
<li><p>The first dimension has a length equal to the number
of input data points.</p></li>
<li><p>The second dimension has a length equal to the number
of components in the Gaussian mixture.</p></li>
</ul>
<p>Each element of the tensor stores a per-data point,
per-component probability density.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.alpha">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">alpha</span></span><a class="headerlink" href="#core.latent.GaussianMixtureModel.alpha" title="Permalink to this definition">#</a></dt>
<dd><p>The Dirichlet alpha determining the uniformity of the
weights of the components in the Gaussian mixture.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.dim">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dim</span></span><a class="headerlink" href="#core.latent.GaussianMixtureModel.dim" title="Permalink to this definition">#</a></dt>
<dd><p>The dimensionality of the Gaussian mixture model.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.dirichlet_constant">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dirichlet_constant</span></span><a class="headerlink" href="#core.latent.GaussianMixtureModel.dirichlet_constant" title="Permalink to this definition">#</a></dt>
<dd><p>The Dirichlet constant.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.log_var">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">log_var</span></span><a class="headerlink" href="#core.latent.GaussianMixtureModel.log_var" title="Permalink to this definition">#</a></dt>
<dd><p>The log-variance of the Gaussian components in the
mixture.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.log_var_dim">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">log_var_dim</span></span><a class="headerlink" href="#core.latent.GaussianMixtureModel.log_var_dim" title="Permalink to this definition">#</a></dt>
<dd><p>The dimensionality of the log-variance.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.log_var_factor">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">log_var_factor</span></span><a class="headerlink" href="#core.latent.GaussianMixtureModel.log_var_factor" title="Permalink to this definition">#</a></dt>
<dd><p>The log-variance factor.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.log_var_params">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">log_var_params</span></span><a class="headerlink" href="#core.latent.GaussianMixtureModel.log_var_params" title="Permalink to this definition">#</a></dt>
<dd><p>The parameters used to inizialize the Gaussian
prior on the log-variances of the components:</p>
<ul class="simple">
<li><p>Mean: <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">log(log_var_params[0])</span></code></p></li>
<li><p>Standard deviation: <code class="docutils literal notranslate"><span class="pre">log_var_params[1]</span></code></p></li>
</ul>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.log_var_prior">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">log_var_prior</span></span><a class="headerlink" href="#core.latent.GaussianMixtureModel.log_var_prior" title="Permalink to this definition">#</a></dt>
<dd><p>The prior over the log-variance.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#core.latent.GaussianMixtureModel.mean" title="Permalink to this definition">#</a></dt>
<dd><p>The means of the Gaussian components in the mixture.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.mean_prior">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean_prior</span></span><a class="headerlink" href="#core.latent.GaussianMixtureModel.mean_prior" title="Permalink to this definition">#</a></dt>
<dd><p>The prior on the means of the Gaussian components
in the mixture.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.n_comp">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n_comp</span></span><a class="headerlink" href="#core.latent.GaussianMixtureModel.n_comp" title="Permalink to this definition">#</a></dt>
<dd><p>The number of components in the Gaussian mixture.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.GaussianMixtureModel.weight">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#core.latent.GaussianMixtureModel.weight" title="Permalink to this definition">#</a></dt>
<dd><p>The weights of the components in the Gaussian mixture.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="core.latent.RepresentationLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">core.latent.</span></span><span class="sig-name descname"><span class="pre">RepresentationLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">values</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.latent.RepresentationLayer" title="Permalink to this definition">#</a></dt>
<dd><p>Class implementing a representation layer accumulating
<code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> gradients.</p>
<dl class="py method">
<dt class="sig sig-object py" id="core.latent.RepresentationLayer.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">values</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dist_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.latent.RepresentationLayer.__init__" title="Permalink to this definition">#</a></dt>
<dd><p>The representations are vectors in an N-dimensional real
space.</p>
<p>If no <code class="docutils literal notranslate"><span class="pre">values</span></code> for the representations are passed, the
representations will be initialized as a two-dimensional
tensor of shape (<code class="docutils literal notranslate"><span class="pre">dist_params[&quot;n_samples&quot;]</span></code>,
<code class="docutils literal notranslate"><span class="pre">dist_params[&quot;dim&quot;]</span></code>) from <code class="docutils literal notranslate"><span class="pre">dist</span></code>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>values</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, optional</span></dt><dd><p>The tensor used to initialize the representations.</p>
<p>If it is not passed, the representations will be
initialized by sampling the distribution specified
with <cite>dist</cite>.</p>
</dd>
<dt><strong>dist</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, {<code class="docutils literal notranslate"><span class="pre">&quot;normal&quot;</span></code>}, default: <code class="docutils literal notranslate"><span class="pre">&quot;normal&quot;</span></code></span></dt><dd><p>The name of the distribution used to sample the
representations, if no <code class="docutils literal notranslate"><span class="pre">values</span></code> are passed.</p>
<p>By default, the distribution is a <code class="docutils literal notranslate"><span class="pre">&quot;normal&quot;</span></code>
distribution.</p>
</dd>
<dt><strong>dist_params</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">dict</span></code>, optional</span></dt><dd><p>A dictionary containing the parameters to sample the
representations from the distribution, if
<code class="docutils literal notranslate"><span class="pre">values</span></code> is not passed.</p>
<p>For any distribution the following keys and associated
parameters must be provided:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;dim&quot;</span></code> : the dimensionality of the representations
to sample from the distribution.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;n_samples&quot;</span></code> : the number of samples to draw from
the distribution.</p></li>
</ul>
<p>If <code class="docutils literal notranslate"><span class="pre">dist</span></code> is <code class="docutils literal notranslate"><span class="pre">&quot;normal&quot;</span></code>, the dictionary must contain
these additional key/value pairs:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code> : the mean of the normal distribution used
to generate the representations.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;stddev&quot;</span></code> : the standard deviation of the normal
distribution used to generate the representations.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.latent.RepresentationLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ixs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.latent.RepresentationLayer.forward" title="Permalink to this definition">#</a></dt>
<dd><p>Forward pass. It returns the representations. You can
select a subset of representations to be returned using
their numerical indexes (<code class="docutils literal notranslate"><span class="pre">ixs</span></code>).</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>ixs</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">list</span></code>, optional</span></dt><dd><p>The indexes of the samples whose representations should
be returned. If not passed, all representations
will be returned.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></dt><dd><p>A tensor containing the representations for the samples
of interest.</p>
<p>This is a 2D tensor where:</p>
<ul class="simple">
<li><p>The first dimension has a length equal to the number
of representations.</p></li>
<li><p>The second dimension has a length equal to the
dimensionality of the representations.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.latent.RepresentationLayer.rescale">
<span class="sig-name descname"><span class="pre">rescale</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.latent.RepresentationLayer.rescale" title="Permalink to this definition">#</a></dt>
<dd><p>Rescale the representations by subtracting the mean
of all representations from each of them and dividing
them by the standard deviation of all representations.</p>
<p>Given <span class="math notranslate nohighlight">\(N\)</span> samples, we can indicate with <span class="math notranslate nohighlight">\(z^{n}\)</span>
the representation of sample <span class="math notranslate nohighlight">\(x^{n}\)</span>. The rescaled
representation <span class="math notranslate nohighlight">\(z^{n}_{rescaled}\)</span> will be, therefore:</p>
<div class="math notranslate nohighlight">
\[z^{n}_{rescaled} = \frac{z^{n} - \bar{z}}{s}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\bar{z}\)</span> is the mean of the representations
and <span class="math notranslate nohighlight">\(s\)</span> is the standard deviation.</p>
<dl class="field-list">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></dt><dd><p>The rescaled representations.</p>
<p>This is a 2D tensor where:</p>
<ul class="simple">
<li><p>The first dimension has a length equal to the number
of representations.</p></li>
<li><p>The second dimension has a length equal to the
dimensionality of the representations.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.RepresentationLayer.dim">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dim</span></span><a class="headerlink" href="#core.latent.RepresentationLayer.dim" title="Permalink to this definition">#</a></dt>
<dd><p>The dimensionality of the representations.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.RepresentationLayer.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#core.latent.RepresentationLayer.mean" title="Permalink to this definition">#</a></dt>
<dd><p>The mean of the distribution the representations
were sampled from, if the user did not pass any
<code class="docutils literal notranslate"><span class="pre">values</span></code> for the representations when initializing the
<code class="docutils literal notranslate"><span class="pre">RepresentationLayer</span></code>.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.RepresentationLayer.n_samples">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n_samples</span></span><a class="headerlink" href="#core.latent.RepresentationLayer.n_samples" title="Permalink to this definition">#</a></dt>
<dd><p>The number of samples for which a representation must
be found.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.RepresentationLayer.stddev">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">stddev</span></span><a class="headerlink" href="#core.latent.RepresentationLayer.stddev" title="Permalink to this definition">#</a></dt>
<dd><p>The standard deviation of the distribution the
representations were sampled from, if the user did
not pass any <code class="docutils literal notranslate"><span class="pre">values</span></code> for the representations
when initializing the <code class="docutils literal notranslate"><span class="pre">RepresentationLayer</span></code>.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.latent.RepresentationLayer.z">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">z</span></span><a class="headerlink" href="#core.latent.RepresentationLayer.z" title="Permalink to this definition">#</a></dt>
<dd><p>The representations.</p>
</dd></dl>

</dd></dl>

</section>
<section id="core-priors">
<h2>core.priors<a class="headerlink" href="#core-priors" title="Permalink to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="core.priors.SoftballPrior">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">core.priors.</span></span><span class="sig-name descname"><span class="pre">SoftballPrior</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">radius</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sharpness</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.priors.SoftballPrior" title="Permalink to this definition">#</a></dt>
<dd><p>Class implementing a “softball” prior.</p>
<p>It is an almost uniform prior for an m-dimensional ball, with
the logistic function making a soft (differentiable) boundary.</p>
<dl class="py method">
<dt class="sig sig-object py" id="core.priors.SoftballPrior.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">radius</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sharpness</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.priors.SoftballPrior.__init__" title="Permalink to this definition">#</a></dt>
<dd><p>Initialize an instance of the “softball” distribution.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>dim</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The dimensionality of the prior.</p>
</dd>
<dt><strong>radius</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">float</span></code></span></dt><dd><p>The radius of the ball.</p>
</dd>
<dt><strong>sharpness</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The sharpness of the “soft” boundary.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.priors.SoftballPrior.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">z</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.priors.SoftballPrior.log_prob" title="Permalink to this definition">#</a></dt>
<dd><p>Return the log-probabilities of the elements of a tensor.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>z</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The input tensor.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></dt><dd><p>The log-probabilities of the input tensor.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.priors.SoftballPrior.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.priors.SoftballPrior.sample" title="Permalink to this definition">#</a></dt>
<dd><p>Get <code class="docutils literal notranslate"><span class="pre">n</span></code> random samples from the softball distribution.
The sampling is uniform from the <code class="docutils literal notranslate"><span class="pre">dim</span></code>-dimensional ball,
and approximate.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>n</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The number of random samples.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></dt><dd><p>The points samples from the softball distribution.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.priors.SoftballPrior.dim">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dim</span></span><a class="headerlink" href="#core.priors.SoftballPrior.dim" title="Permalink to this definition">#</a></dt>
<dd><p>The dimensionality of the softball prior.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.priors.SoftballPrior.radius">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">radius</span></span><a class="headerlink" href="#core.priors.SoftballPrior.radius" title="Permalink to this definition">#</a></dt>
<dd><p>The radius of the ball.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.priors.SoftballPrior.sharpness">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">sharpness</span></span><a class="headerlink" href="#core.priors.SoftballPrior.sharpness" title="Permalink to this definition">#</a></dt>
<dd><p>The sharpness of the “soft” boundary of the ball.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="core.priors.GaussianPrior">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">core.priors.</span></span><span class="sig-name descname"><span class="pre">GaussianPrior</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stddev</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.priors.GaussianPrior" title="Permalink to this definition">#</a></dt>
<dd><p>Class implementing a Gaussian prior used to initialize
the Gaussian mixture model’s means.</p>
<dl class="py method">
<dt class="sig sig-object py" id="core.priors.GaussianPrior.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stddev</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.priors.GaussianPrior.__init__" title="Permalink to this definition">#</a></dt>
<dd><p>Initialize an instance of the Gaussian distribution.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>dim</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The dimensionality of the prior.</p>
</dd>
<dt><strong>mean</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">float</span></code></span></dt><dd><p>The mean of the Gaussian distribution.</p>
</dd>
<dt><strong>stddev</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">float</span></code></span></dt><dd><p>The standard deviation of the Gaussian distribution.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.priors.GaussianPrior.log_prob">
<span class="sig-name descname"><span class="pre">log_prob</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.priors.GaussianPrior.log_prob" title="Permalink to this definition">#</a></dt>
<dd><p>Return the log probabilities of a tensor.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></span></dt><dd><p>The input tensor.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></dt><dd><p>The log probabilities of the input tensor.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="core.priors.GaussianPrior.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_samples</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#core.priors.GaussianPrior.sample" title="Permalink to this definition">#</a></dt>
<dd><p>Sample from the Gaussian distribution.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>n_samples</strong><span class="classifier"><code class="docutils literal notranslate"><span class="pre">int</span></code></span></dt><dd><p>The number of samples.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code></dt><dd><p>The points sampled from the Gaussian distribution.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.priors.GaussianPrior.dim">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dim</span></span><a class="headerlink" href="#core.priors.GaussianPrior.dim" title="Permalink to this definition">#</a></dt>
<dd><p>The dimensionality of the Gaussian prior.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.priors.GaussianPrior.dist">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dist</span></span><a class="headerlink" href="#core.priors.GaussianPrior.dist" title="Permalink to this definition">#</a></dt>
<dd><p>The distribution associated with the prior.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.priors.GaussianPrior.mean">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">mean</span></span><a class="headerlink" href="#core.priors.GaussianPrior.mean" title="Permalink to this definition">#</a></dt>
<dd><p>The mean of the Gaussian distribution.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="core.priors.GaussianPrior.stddev">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">stddev</span></span><a class="headerlink" href="#core.priors.GaussianPrior.stddev" title="Permalink to this definition">#</a></dt>
<dd><p>The standard deviation of the Gaussian distribution.</p>
</dd></dl>

</dd></dl>

</section>
</section>


                </article>
              
              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="Command-line%20interface.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Command-line interface</p>
      </div>
    </a>
    <a class="right-next"
       href="utils.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The utils subpackage</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-dataclasses">core.dataclasses</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core.dataclasses.GeneExpressionDataset"><code class="docutils literal notranslate"><span class="pre">GeneExpressionDataset</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.dataclasses.GeneExpressionDataset.__init__"><code class="docutils literal notranslate"><span class="pre">GeneExpressionDataset.__init__()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.dataclasses.GeneExpressionDataset.df"><code class="docutils literal notranslate"><span class="pre">GeneExpressionDataset.df</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.dataclasses.GeneExpressionDataset.mean_exp"><code class="docutils literal notranslate"><span class="pre">GeneExpressionDataset.mean_exp</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.dataclasses.GeneExpressionDataset.n_genes"><code class="docutils literal notranslate"><span class="pre">GeneExpressionDataset.n_genes</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.dataclasses.GeneExpressionDataset.n_samples"><code class="docutils literal notranslate"><span class="pre">GeneExpressionDataset.n_samples</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-decoder">core.decoder</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core.decoder.NBLayer"><code class="docutils literal notranslate"><span class="pre">NBLayer</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.decoder.NBLayer.__init__"><code class="docutils literal notranslate"><span class="pre">NBLayer.__init__()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.decoder.NBLayer.forward"><code class="docutils literal notranslate"><span class="pre">NBLayer.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.decoder.NBLayer.log_prob"><code class="docutils literal notranslate"><span class="pre">NBLayer.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.decoder.NBLayer.log_prob_mass"><code class="docutils literal notranslate"><span class="pre">NBLayer.log_prob_mass()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.decoder.NBLayer.loss"><code class="docutils literal notranslate"><span class="pre">NBLayer.loss()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.decoder.NBLayer.rescale"><code class="docutils literal notranslate"><span class="pre">NBLayer.rescale()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.decoder.NBLayer.sample"><code class="docutils literal notranslate"><span class="pre">NBLayer.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.decoder.NBLayer.activation"><code class="docutils literal notranslate"><span class="pre">NBLayer.activation</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.decoder.NBLayer.dim"><code class="docutils literal notranslate"><span class="pre">NBLayer.dim</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.decoder.NBLayer.log_r"><code class="docutils literal notranslate"><span class="pre">NBLayer.log_r</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core.decoder.Decoder"><code class="docutils literal notranslate"><span class="pre">Decoder</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.decoder.Decoder.__init__"><code class="docutils literal notranslate"><span class="pre">Decoder.__init__()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.decoder.Decoder.forward"><code class="docutils literal notranslate"><span class="pre">Decoder.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-latent">core.latent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.__init__"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.__init__()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.component_sample"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.component_sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.forward"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.get_distribution"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.get_distribution()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.get_mixture_probs"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.get_mixture_probs()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.get_prior_log_prob"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.get_prior_log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.log_prob"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.sample"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.sample_new_points"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.sample_new_points()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.sample_probs"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.sample_probs()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.alpha"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.alpha</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.dim"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.dim</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.dirichlet_constant"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.dirichlet_constant</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.log_var"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.log_var</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.log_var_dim"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.log_var_dim</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.log_var_factor"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.log_var_factor</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.log_var_params"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.log_var_params</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.log_var_prior"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.log_var_prior</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.mean"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.mean_prior"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.mean_prior</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.n_comp"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.n_comp</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.GaussianMixtureModel.weight"><code class="docutils literal notranslate"><span class="pre">GaussianMixtureModel.weight</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.RepresentationLayer"><code class="docutils literal notranslate"><span class="pre">RepresentationLayer</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.RepresentationLayer.__init__"><code class="docutils literal notranslate"><span class="pre">RepresentationLayer.__init__()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.RepresentationLayer.forward"><code class="docutils literal notranslate"><span class="pre">RepresentationLayer.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.RepresentationLayer.rescale"><code class="docutils literal notranslate"><span class="pre">RepresentationLayer.rescale()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.RepresentationLayer.dim"><code class="docutils literal notranslate"><span class="pre">RepresentationLayer.dim</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.RepresentationLayer.mean"><code class="docutils literal notranslate"><span class="pre">RepresentationLayer.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.RepresentationLayer.n_samples"><code class="docutils literal notranslate"><span class="pre">RepresentationLayer.n_samples</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.RepresentationLayer.stddev"><code class="docutils literal notranslate"><span class="pre">RepresentationLayer.stddev</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.latent.RepresentationLayer.z"><code class="docutils literal notranslate"><span class="pre">RepresentationLayer.z</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-priors">core.priors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core.priors.SoftballPrior"><code class="docutils literal notranslate"><span class="pre">SoftballPrior</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.priors.SoftballPrior.__init__"><code class="docutils literal notranslate"><span class="pre">SoftballPrior.__init__()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.priors.SoftballPrior.log_prob"><code class="docutils literal notranslate"><span class="pre">SoftballPrior.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.priors.SoftballPrior.sample"><code class="docutils literal notranslate"><span class="pre">SoftballPrior.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.priors.SoftballPrior.dim"><code class="docutils literal notranslate"><span class="pre">SoftballPrior.dim</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.priors.SoftballPrior.radius"><code class="docutils literal notranslate"><span class="pre">SoftballPrior.radius</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.priors.SoftballPrior.sharpness"><code class="docutils literal notranslate"><span class="pre">SoftballPrior.sharpness</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core.priors.GaussianPrior"><code class="docutils literal notranslate"><span class="pre">GaussianPrior</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.priors.GaussianPrior.__init__"><code class="docutils literal notranslate"><span class="pre">GaussianPrior.__init__()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.priors.GaussianPrior.log_prob"><code class="docutils literal notranslate"><span class="pre">GaussianPrior.log_prob()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.priors.GaussianPrior.sample"><code class="docutils literal notranslate"><span class="pre">GaussianPrior.sample()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.priors.GaussianPrior.dim"><code class="docutils literal notranslate"><span class="pre">GaussianPrior.dim</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.priors.GaussianPrior.dist"><code class="docutils literal notranslate"><span class="pre">GaussianPrior.dist</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.priors.GaussianPrior.mean"><code class="docutils literal notranslate"><span class="pre">GaussianPrior.mean</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core.priors.GaussianPrior.stddev"><code class="docutils literal notranslate"><span class="pre">GaussianPrior.stddev</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div class="tocsection sourcelink">
    <a href="_sources/core.rst.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner"></div>
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      © Copyright 2023, Valentina Sora.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://sphinx-doc.org/">Sphinx</a> 6.1.3.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.13.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>