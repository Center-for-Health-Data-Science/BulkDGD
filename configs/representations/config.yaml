# Section containing the options for loading the gene expression data
data:
  
  # The data will be loaded in batches of 'batch_size' size
  batch_size: 8
  
  # Whether to shuffle the data when loading them
  shuffle: True
 

# Dimensionality of the latent (= representation) space
dim_latent: 50


# Section containing the options for setting up the decoder
dec:

  # PyTorch file containing the parameters of the trained decoder.
  # Make sure that the file contains a decoder whose parameters
  # fit the architecture specified in the 'options' section.
  pth_file: ""
  
  # Section containing the options to initialize the decoder
  options:

    # The number of neurons in the input layer of the decoder
    # does not need to be specified since it corresponds to
    # the dimensionality of the latent space
    
    # Number of neurons in the first hidden layer
    n_neurons_hidden1: 500
    
    # Number of neurons in the second hidden layer
    n_neurons_hidden2: 8000
    
    # Number of neurons in the output layer (= number of
    # genes the decoder has been trained on)
    n_neurons_out: 16883
    
    # Initial "number of successes" (r) for the negative
    # binomial distributions modeling the output layer
    r_init: 2
    
    # Type of scaling used in the negative binomial
    # distributions. Choices are: 'library', 'total_count',
    # 'mean', and 'median', with 'library' and 'total_count'
    # determining the selection of the sigmoid activation function,
    # and 'mean' and 'median' meaning a the relu activation
    # function will be used
    scaling_type: "mean"


# Section containing the options for setting up the Gaussian
# mixture model (GMM)
gmm:

  # PyTorch file containing the parameters of the trained GMM.
  # Make sure that the file contains a GMM whose parameters
  # fit the architecture specified in the 'options' section.
  pth_file: ""

  # Section containing the options to initialize the Gaussian
  # mixture model (GMM)
  options:

    # Number of components in the mixture
    n_mix_comp: 45

    # Type of covariance matrix. Choices are 'diagonal',
    # 'fixed', and 'isotropic'
    cm_type: "diagonal"

    # Parameters to initialize the Gaussian prior
    # on the log-variants of the components:
    # - mean: 2 * log(logbeta_params[0])
    # - stddev: logbeta_params[1]
    logbeta_params: !!seq [0.1, 1]

    # Alpha of the Dirichlet distribution determining
    # the uniformity of the weights of the components
    # in the mixture
    alpha: 5

  # Prior on the means of the Gaussians
  mean_prior:

    # Type of prior (e.g., "softball")
    type: "softball"

    # Section containing the options for the prior
    # (they vary according to the type of prior)
    options:

      # For the softball prior, radius of the ball
      radius: 7

      # For the softball prior, sharpness of the soft
      # boundary
      sharpness: 10


# Section containing the options for setting up the
# representation layer
rep_layer:

  # PyTorch file containing the parameters of the trained
  # representation layer. Make sure that the file contains
  # a representation layer whose parameters fit the
  # architecture specified in the 'options' section.
  pth_file: ""

  # Section containing the options to initialize the
  # representation layer
  options:

    # Number of samples the representation layer
    # has been trained on
    n_samples: 17072


# Section containing the options for the different
# rounds of optimization used when finding the
# best representations for new samples
optimization:

  # Section containing the options for the initial
  # round of optimization
  opt1:

    # Number of epochs
    epochs: 10

    # Type of optimizer (e.g., "adam")
    type: "adam"

    # Section containing the options for the optimizer
    # (they vary according to the type of optimizer)
    options:

      # Learning rate
      lr: 0.01

      # Weight decay
      weight_decay: 0

      # Betas
      betas: !!seq [0.5, 0.9]

  # Section containing the options for further
  # optimization
  opt2:

    # Number of epochs
    epochs: 50

    # Type of optimizer (e.g., "adam")
    type: "adam"

    # Section containing the options for the optimizer
    # (they vary according to the type of optimizer)
    options:

      # Learning rate
      lr: 0.01

      # Weight decay
      weight_decay: 0

      # Betas
      betas: !!seq [0.5, 0.9]