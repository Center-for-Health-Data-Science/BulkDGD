

#---------------------- Gaussian mixture model -----------------------#


# Section containing the options for setting up the Gaussian
# mixture model (GMM)
gmm:

  # PyTorch file containing the parameters of the trained GMM.
  # Make sure that the file contains a GMM whose parameters
  # fit the architecture specified in the 'options' section.
  pth_file: default

  # Section containing the options to initialize the Gaussian
  # mixture model (GMM)
  options:

    # Dimensionality of the Gaussian mixture model
    dim: 50

    # Number of components in the mixture
    n_comp: 45

    # Type of covariance matrix. Choices are 'diagonal',
    # 'fixed', and 'isotropic'
    cm_type: diagonal

    # Parameters to initialize the Gaussian prior
    # on the log-variants of the components:
    # - mean: 2 * log(logbeta_params[0])
    # - stddev: logbeta_params[1]
    log_var_params: [0.1, 1]

    # Alpha of the Dirichlet distribution determining
    # the uniformity of the weights of the components
    # in the mixture
    alpha: 5

  # Prior on the means of the Gaussians
  mean_prior:

    # Type of prior (e.g., "softball")
    type: softball

    # Section containing the options for the prior
    # (they vary according to the type of prior)
    options:

      # For the softball prior, radius of the ball
      radius: 7

      # For the softball prior, sharpness of the soft
      # boundary
      sharpness: 10


#------------------------------ Decoder ------------------------------#


# Section containing the options for setting up the decoder
dec:

  # PyTorch file containing the parameters of the trained decoder.
  # Make sure that the file contains a decoder whose parameters
  # fit the architecture specified in the 'options' section
  pth_file: default
  
  # Section containing the options to initialize the decoder
  options:

    # Number of neurons in the input layer. They must correspond
    # to the dimensionality of the latent space (Gaussian mixture
    # model and representation layer)
    n_neurons_input: 50
    
    # Number of neurons in the first hidden layer
    n_neurons_hidden1: 500
    
    # Number of neurons in the second hidden layer
    n_neurons_hidden2: 8000
    
    # Number of neurons in the output layer (= number of
    # genes the decoder has been trained on)
    n_neurons_output: 16883
    
    # Initial "number of successes" (r) for the negative
    # binomial distributions modeling the output layer
    r_init: 2

    # Name of the activation function to be used in the
    # output layer
    activation_output: softplus
    